## 使用模型并行进行训练

## 简介

研究表明，随着模型规模的扩大，往往能够取得更好的任务性能。然而，随着模型采用更深、更宽的网络层，模型的参数规模也随之增长，甚至是超过计算设备的显存或者内存容量。

使用模型并行可以将模型参数放置到多个计算设备，从而降低单个计算设备的显存或者内存消耗，使得大规模神经网络模型的训练成为可能。理论上讲，使用足够多的计算设备可以训练任意规模的模型。

本文档介绍如何使用飞桨的底层集合通信API（如allreduce、alltoall）等实现模型并行训练。本文档使用下图所示的卷积网络模型作为说明示例：该模型包含三层卷积层合两层全连接层。具体地，卷积层采用数据并行，全连接层采用模型并行。



![示例模型](img/model_parallel_1.png)



## 工作流程

仍然以上图为例，说明训练过程：

